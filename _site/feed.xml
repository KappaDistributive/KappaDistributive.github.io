<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-06-17T00:43:21+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Stefan Mesken</title><subtitle>Data Science</subtitle><author><name>Stefan Mesken</name></author><entry><title type="html">An Hommage to Feynman’s Technique</title><link href="http://localhost:4000/machine%20learning/Feynman-technique/" rel="alternate" type="text/html" title="An Hommage to Feynman's Technique" /><published>2019-06-17T00:00:00+00:00</published><updated>2019-06-17T00:00:00+00:00</updated><id>http://localhost:4000/machine%20learning/Feynman-technique</id><content type="html" xml:base="http://localhost:4000/machine%20learning/Feynman-technique/">&lt;h1 id=&quot;fragile-knowledge&quot;&gt;Fragile Knowledge&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;I don’t know what’s the matter with people: they don’t learn by
understanding; they learn by some other way - by rote, or
something. Their knowledge is so fragile!&lt;/p&gt;

  &lt;p&gt;– Richard P. Feynman&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One key to successfully mastering a new subject, I believe, is to
connect it to something that you are already deeply familiar
with. This idea is as old as human learning and a common quality of
great lecturers is that they fully embrace its logical conclusion:
When introducing an audience to any novel concept, start out by
conveying the main ideas in analogies and metaphors rather than
drowning them in complexities and details.&lt;/p&gt;

&lt;p&gt;And while people tend to agree that this is a good quality in a
teacher, they often deprive themselves of this luxury when learning on
their own. Admittedly, explaining a new idea well to yourself requires
a substantial amount of additional effort. However, I am convinced
that this overhead is not only economical but of fundamental
importance if you ever plan to master a new set skills (as opposed to
gaining the mostly worthless knowledge of sparsely connected facts).&lt;/p&gt;

&lt;h1 id=&quot;case-study-understanding-neural-networks&quot;&gt;Case Study: Understanding Neural Networks&lt;/h1&gt;

&lt;p&gt;So, if you are still with me, I’d like to demonstrate how this might
look like when first learning about neural networks. Basically I’ll
try to reconstruct the inner monologue that went through my head when
I just first learned about neural networks.&lt;/p&gt;

&lt;p&gt;“Neural networks are basically &lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_graph&quot;&gt;directed
graphs&lt;/a&gt; with
metadata. Nodes represent neurons, edges represent the data-flow
between those neurons and the metadata comes in two flavours:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The weight attached to every edge determines how much of an
influence its source neuron has in the overall network and&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An activation function, attached to each neuron, that determines the
‘shape’ of this neuron’s discharge pattern. &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So far, so good. But what does that mean? How can a neural network
capture information?”&lt;/p&gt;

&lt;p&gt;Stepping out of character for a second: It’s here that you can make
life very difficult for yourself. Both leaving this question
unanswered and jumping into some complex example
(e.g. &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt;) are, in my opinion,
terrible mistakes. You want to build a simple example, ideally relying
only on yourself, for two reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First and most importantly: This is a litmus test. If you are not
able to build a basic example of the concept you’ve just learned
about, you haven’t understood the basics yet and need to revisit
them. By forcing yourself to sit down and actually spelling out a very
simple example in detail, you’re making sure that your previous gained
confidence is justified and that you’re not just fooling yourself.
    &lt;blockquote&gt;
      &lt;p&gt;The first principle is that you must not fool yourself – and you are
the easiest person to fool.&lt;/p&gt;

      &lt;p&gt;– Richard P. Feynman&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you want to master any subject, it is my firm opinion that you
must carry around a large set of examples in your head. These
examples will not only guide your intuition but they will also
serve as a more and more elaborate testing ground for new ideas. In
the future, if someone tells you about some new fancy concept (that
someone might be you), you can refer to an appropriate example in
your head and see how it would influence that. If the results don’t
make sense, something’s wrong! Either you’ve misunderstood what the
other party is telling you or there is a mistake. If you want this
conversation to remain meaningful, you need to fix that! This is
how I, and how I believe most of my colleagues, are able to build
an appropriate intuition about complex concepts that often run
counter to any experience you might have had in your daily life.&lt;/p&gt;

    &lt;p&gt;Building a simple example from scratch is your first step
toward assembling your internal database.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Okay, let’s return to the inner monologue:&lt;/p&gt;

&lt;p&gt;“Is there anything I already know about that could easily be realized
as a neural network? Actually, yes: Linear functions are of the form
&lt;script type=&quot;math/tex&quot;&gt;f(x) = a \cdot x + b&lt;/script&gt; and they are represented by the following
neural network: &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;figure style=&quot;align: center&quot;&gt;
  &lt;img src=&quot;/images/diagrams/nn_linear_function.png&quot; style=&quot;max-width: 400px;&quot; alt=&quot;a neural network that represents a linear function&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Great, but how about logical connections? If artificially neural
networks are supposed to model actual neural networks, they certainly
must be able to capture logic gates, right? Right!&lt;/p&gt;

&lt;figure style=&quot;align: center&quot;&gt;
  &lt;img src=&quot;/images/diagrams/nn_not.png&quot; style=&quot;max-width: 400px;&quot; alt=&quot;a neural network that represents logical negation&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;\chi = \chi_{[0, \infty)}&lt;/script&gt; is the activation function of
&lt;script type=&quot;math/tex&quot;&gt;a_1&lt;/script&gt; and all you need to know is that &lt;script type=&quot;math/tex&quot;&gt;\chi(z) = 0&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
z
&lt; 0 %]]&gt;&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\chi(z) = 1&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;z \ge 0&lt;/script&gt;. So, if we set &lt;script type=&quot;math/tex&quot;&gt;x_1 =
0&lt;/script&gt;, we get that &lt;script type=&quot;math/tex&quot;&gt;a_1 = \chi(w_0 \cdot x_0 + w_1 \cdot x_1) =
\chi(0 \cdot 1 + (-1) \cdot 0) = \chi(0) = 0&lt;/script&gt;. On the other hand, if we let
&lt;script type=&quot;math/tex&quot;&gt;x_1 = 1&lt;/script&gt;, we get that &lt;script type=&quot;math/tex&quot;&gt;a_1 = \chi(0 \cdot 1 + (-1) \cdot 1) = \chi(-1) =
0&lt;/script&gt;. So this neural network does indeed represent &lt;script type=&quot;math/tex&quot;&gt;\mathrm{NOT}(x_1)&lt;/script&gt;!”&lt;/p&gt;

&lt;p&gt;At this point, you might want to cook up a few more examples and in an
earlier tweet I demonstrated how to represent other basic logic gates
(AND, OR, NOR, XOR, XNOR) as neural networks as well. I’d encourage
you to try it yourself first and then &lt;a href=&quot;https://twitter.com/mesken_stefan/status/1138694458470539264&quot;&gt;hop over to
Twitter&lt;/a&gt;
and compare your results with mine. &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Once you’ve done that, you may notice what I’ve noticed: Not only can
neural networks represent logic gates. &lt;em&gt;Neural networks are nothing
more than logic gates&lt;/em&gt;, with a few minor adjustments:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Inputs are allowed to take on any real number as value, not just &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We add a restriction that a node’s value is computed by a weighted,
linear combination of the values to its left followed by an
activation function and&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We add an activation functions to our nodes. &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;”&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It may not seem that way, but this little detour is incredibly
beneficial to your overall learning strategy. &lt;strong&gt;Ideas and memories die
in a vacuum&lt;/strong&gt;. If you want them to become lasting and meaningful, you
need to connect to as many previously established ideas as
possible. Don’t believe me? Then tell me: What did you have for dinner
last Wednesday? Unless you are able to connect last Wednesday’s dinner
to something more meaningful (maybe you happened to be on a first
date), most of us will struggle mightily with this very simple
question. And if I ask you about your lunch a year ago, there’s
basically no chance you’ll be able to remember. On the other hand, if
I ask you what you did last Christmas (or even better: On your wedding
day if you happen to be married), the task becomes much easier. You
either connect ideas and memories or you will lose them –
quickly. Not only that: Sparsely connected ideas (what Feynman calls
‘fragile knowledge’) are basically worthless as they’re not readily
available to be used in similar, but slightly different, settings.&lt;/p&gt;

&lt;h1 id=&quot;what-you-should-take-away-from-this&quot;&gt;What you should take away from this&lt;/h1&gt;

&lt;p&gt;If you make a habit of &lt;em&gt;always&lt;/em&gt; explaining new ideas to yourself until
you’ve completely broken them down to related concepts, that you’ve
already mastered, you’ll never have to learn or memorize anything
truly novel again. And, over time, you’ll gain firm, deeply connected,
foundational knowledge about seemingly unrelated concepts that will
not only help you to learn other concepts faster and more efficient
but, ultimately, also allow you to solve problems in surprising,
efficient and truly ingenious ways. Being able to do that is, in a
nutshell, the mark of a true expert.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;I think it’s helpful to point out that this is one of the ways
that artificial neural networks differ from biological
ones. Biological neurons either fire or they don’t – so, in a
way, the only activation function they’re allowed to have is the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Indicator_function&quot;&gt;characteristic
function&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;\chi_{(-\infty, a]}&lt;/script&gt; for some number &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;This follows Andrew Ng’s handy convention that any neuron/weight with
a &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; as subscript represents a bias component – independent of the input. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;In case you don’t want to use Twitter, you can find my solution &lt;a href=&quot;http://localhost:4000/images/diagrams/nn_logic_gates.png&quot;&gt;here&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;This point, in theory, is incredibly powerful. In fact, it’s too
powerful. Since neural networks are intended to approximate some
function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; to begin with, we could always achieve this by
taking &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; as our activation function. In practice, however,
&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is unknown, we only allow activation functions from a very
restrictive, simple set of functions and their specific choice, in
many cases, turns out to be surprisingly irrelevant. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stefan Mesken</name></author><category term="Machine Learning" /><category term="neural networks" /><category term="machine learning" /><category term="human learning" /><summary type="html">Fragile Knowledge</summary></entry><entry><title type="html">TensorFlow Without Tears</title><link href="http://localhost:4000/machine%20learning/tensorflow-without-tears/" rel="alternate" type="text/html" title="TensorFlow Without Tears" /><published>2019-06-11T00:00:00+00:00</published><updated>2019-06-11T00:00:00+00:00</updated><id>http://localhost:4000/machine%20learning/tensorflow-without-tears</id><content type="html" xml:base="http://localhost:4000/machine%20learning/tensorflow-without-tears/">&lt;p&gt;Life is pretty crazy right now. Finishing a PhD, changing from set theory to applied data science, forming connections to other data scientists, learning as much about machine learning (both theory and practice) as I possibly can, finding a new apartment and preparing to move, dealing with German bureaucracy… This doesn’t leave a lot of spare time. Truth be told, it doesn’t leave any.&lt;/p&gt;

&lt;p&gt;I’m not complaining – the last couple of months have been immensely enjoyable (well, except for the bureaucracy part). But being pressed for time also made me decide to postpone my planned blog series on the importance of probability distributions in machine learning as I’m currently lacking the spare mental capacity to do this rather broad, open-ended topic justice.&lt;/p&gt;

&lt;h1 id=&quot;tensorflow-2&quot;&gt;TensorFlow 2&lt;/h1&gt;

&lt;p&gt;Instead, to finally return the ball of machine learning conversation to my readers, I’d like to share my excitement about &lt;a href=&quot;https://www.tensorflow.org/beta/&quot;&gt;TensorFlow 2&lt;/a&gt;. The beta of TF2 has been announced 4 days ago and while I didn’t take part in the alpha at all, I’ve set aside a bit of time over the weekend to play around with the newly released beta. And it’s been incredibly exciting!&lt;/p&gt;

&lt;p&gt;See, what often sets me apart in a group of people is that I really like to get down to the nuts and bolts of anything I lay my hands on. As a young child I’ve disassembled pretty much every electronic device in our household, I’ve worked on &lt;a href=&quot;https://en.wikipedia.org/wiki/Trabant&quot;&gt;charmingly simple cars&lt;/a&gt; by the time I graduated from kindergarten, I’ve spent most of my childhood in the workshop of my grandfather. This hands-on, detail-focused, borderline obsessive personality trait never left me.&lt;/p&gt;

&lt;p&gt;And it’s precisely what made me fall in love with TF1 when I first discovered it. TF1, at least to me, isn’t really a machine learning framework at heart. Admittedly, it has a bunch of utilities built into it that are useful in machine learning applications, but what TF1 does at its core is to provide a framework for the manipulation of dataflow graphs. This philosophy makes TF1 very versatile, performant and transparent. But it also sets up a learning curve that, especially to someone without a background in mathematics or computer science, looks more like a learning wall. And like so many other learning walls (vi anyone?), it naturally turns away a lot of people. Especially those people that I’ve benefited the most from during my entire life: People who, unlike me, don’t care much about bolts and nuts but who naturally focus much more on the greater picture, who provide a sense of style, a desire for polish that benefits the entire community.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt; has done a lot to attract a more diverse group of users to TensorFlow 1. To quote their &lt;a href=&quot;https://keras.io/why-use-keras/&quot;&gt;FAQ&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Keras prioritizes developer experience&lt;/strong&gt;&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent &amp;amp; simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.&lt;/li&gt;
    &lt;li&gt;This makes Keras easy to learn and easy to use. As a Keras user, you are more productive, allowing you to try more ideas than your competition, faster – which in turn helps you win machine learning competitions.&lt;/li&gt;
    &lt;li&gt;This ease of use does not come at the cost of reduced flexibility: because Keras integrates with lower-level deep learning languages (in particular TensorFlow), it enables you to implement anything you could have built in the base language. In particular, as tf.keras, the Keras API integrates seamlessly with your TensorFlow workflows.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;This amazing body of work has enabled TF2 to pull off what so rarely seems attainable: By embracing Keras’ philosophy, TF2 became a piece of software that is both user-friendly and still provides all the functionality, all the performance and all the transparency any user could ask for. This almost never happens but when it does, it makes me really, really happy.&lt;/p&gt;

&lt;h1 id=&quot;hello-im-tensorflow&quot;&gt;Hello, I’m TensorFlow.&lt;/h1&gt;

&lt;p&gt;In an effort to invite as many people as possible to share my excitement about TF2 and join the discussion, I’d like to end this post with a very simple regression task completed in TensorFlow 2. We aim to approximate the function &lt;script type=&quot;math/tex&quot;&gt;\sin \colon [0, 2 \pi] \to [-1, 1], x \mapsto \sin (x)&lt;/script&gt; with a neural network. You can find the &lt;a href=&quot;https://colab.research.google.com/drive/1b1Lil2bfpT--axFKgd2z-2LeIkCv0z5Y&quot;&gt;interactive Jupyter notebook over at Google Colaboratory&lt;/a&gt; or just continue reading for a slighly more verbose version.&lt;/p&gt;

&lt;p&gt;The first part of any machine learning project is to load our dependencies.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nightly&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preview&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we load our data. In this case, to keep things as simple as possible, we will simply create the sine function on the interval &lt;script type=&quot;math/tex&quot;&gt;[0, 2 \pi]&lt;/script&gt; and approximate it.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When possible, it’s always a good idea to look at a visual representation of your data. This serves as a baseline sanity check to ensure there is no immediately obvious problem.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure style=&quot;align: center&quot;&gt;
  &lt;img src=&quot;/images/diagrams/sine.png&quot; style=&quot;max-width: 400px;&quot; alt=&quot;graph of the sine function&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;That looks okay, so let’s continue by building our neural network. The integration of Keras makes this very simple:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Build a neural network with 2 hidden layers, each of size 128
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'linear'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Compiling the model is similarly trivial.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mean_squared_error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, let’s fit the neural network to our training data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Train on 10000 samples&lt;/p&gt;

  &lt;p&gt;Epoch 1/5
10000/10000 [==============================] - 1s 72us/sample - loss: 0.1698&lt;/p&gt;

  &lt;p&gt;Epoch 2/5
10000/10000 [==============================] - 1s 61us/sample - loss: 0.0904&lt;/p&gt;

  &lt;p&gt;Epoch 3/5
10000/10000 [==============================] - 1s 58us/sample - loss: 0.0415&lt;/p&gt;

  &lt;p&gt;Epoch 4/5
10000/10000 [==============================] - 1s 53us/sample - loss: 0.0109&lt;/p&gt;

  &lt;p&gt;Epoch 5/5
10000/10000 [==============================] - 1s 52us/sample - loss: 0.0023&lt;/p&gt;

  &lt;p&gt;&amp;lt;tensorflow.python.keras.callbacks.History at 0x7fa30305c5f8&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That’s it! We’ve created and successfully trained a neural network to approximate the sine function on &lt;script type=&quot;math/tex&quot;&gt;[0, 2 \pi]&lt;/script&gt; and the training output seems encouraging. All that’s left to do is to visualize how well we’ve done:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'model prediction'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure style=&quot;align: center&quot;&gt;
  &lt;img src=&quot;/images/diagrams/since_prediction.png&quot; style=&quot;max-width: 400px;&quot; alt=&quot;prediction of the sine function&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Not bad, not bad at all! The interval &lt;script type=&quot;math/tex&quot;&gt;[5,2 \pi]&lt;/script&gt; needs some work (and I encourage you to think about why our model performed so much worse on that section) but other than that, I’d say that our model does know how to create a sine wave.&lt;/p&gt;

&lt;p&gt;I’ve decided to give a few hints as to why our model performs worse on the tail end of our data set: Consider the following animation of the learning process&lt;/p&gt;

&lt;figure style=&quot;align: center&quot;&gt;
  &lt;img src=&quot;/images/diagrams/adam_animation.gif&quot; style=&quot;max-width: 400px;&quot; alt=&quot;graph of the sine function&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;and keep in mind that we are using the ‘Adam’ optimizer.&lt;/p&gt;</content><author><name>Stefan Mesken</name></author><category term="Machine Learning" /><category term="machine learning" /><summary type="html">Life is pretty crazy right now. Finishing a PhD, changing from set theory to applied data science, forming connections to other data scientists, learning as much about machine learning (both theory and practice) as I possibly can, finding a new apartment and preparing to move, dealing with German bureaucracy… This doesn’t leave a lot of spare time. Truth be told, it doesn’t leave any.</summary></entry><entry><title type="html">How To Beat Kaggle (the Easy Way)</title><link href="http://localhost:4000/machine%20learning/how-to-beat-kaggle-(the-easy-way)/" rel="alternate" type="text/html" title="How To Beat Kaggle (the Easy Way)" /><published>2019-05-25T00:00:00+00:00</published><updated>2019-05-25T00:00:00+00:00</updated><id>http://localhost:4000/machine%20learning/how-to-beat-kaggle-(the-easy-way)</id><content type="html" xml:base="http://localhost:4000/machine%20learning/how-to-beat-kaggle-(the-easy-way)/">&lt;p&gt;A few nights ago, I found myself tinkering with the &lt;a href=&quot;https://www.kaggle.com/c/titanic&quot;&gt;Titanic data set on Kaggle&lt;/a&gt; and couldn’t help but notice the number of people with a &lt;a href=&quot;https://www.kaggle.com/c/titanic/leaderboard&quot;&gt;perfect score&lt;/a&gt; – many of whom have a single entry.&lt;/p&gt;

&lt;p&gt;So, I thought to myself: “Clearly, they must be cheating. But how do you cheat efficiently?”&lt;/p&gt;

&lt;h2 id=&quot;a-mathematicians-perspective&quot;&gt;A Mathematician’s Perspective&lt;/h2&gt;

&lt;p&gt;The Kaggle competition ‘Titanic: Machine Learning from Disaster’ (and in fact any classification competition on Kaggle) can be modelled as follows:&lt;/p&gt;

&lt;p&gt;Kaggle asks you to find a secret &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-dimensional vector &lt;script type=&quot;math/tex&quot;&gt;\vec{k} = (k_1, k_2, \ldots, k_n)&lt;/script&gt; (with &lt;script type=&quot;math/tex&quot;&gt;n=418&lt;/script&gt; for the Titanic data set) where each number &lt;script type=&quot;math/tex&quot;&gt;k_i&lt;/script&gt; is either &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; (the passenger with ID &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; didn’t survive) or &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; (the passenger with ID &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; did survive). So all we really have to do is to guess &lt;script type=&quot;math/tex&quot;&gt;\vec{k}&lt;/script&gt; – no need for fancy machine learning techniques!&lt;/p&gt;

&lt;p&gt;There are &lt;script type=&quot;math/tex&quot;&gt;2^n&lt;/script&gt; many possible values for &lt;script type=&quot;math/tex&quot;&gt;\vec{k}&lt;/script&gt; and guessing all of them would be practically impossible. Fortunately, there’s one more ingredient to Kaggle that we can exploit: If we submit a guess &lt;script type=&quot;math/tex&quot;&gt;\vec{g} = (g_1, \ldots, g_n)&lt;/script&gt; to Kaggle, it will return a score – the number of correct guesses. I.e. the number of &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;s such that &lt;script type=&quot;math/tex&quot;&gt;g_i = k_i&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;a-naive-approach&quot;&gt;A Naive Approach&lt;/h2&gt;

&lt;p&gt;This allows for a naive solution in (at most) &lt;script type=&quot;math/tex&quot;&gt;n+1&lt;/script&gt; many guesses: First guess &lt;script type=&quot;math/tex&quot;&gt;\vec{g}_0 = (0,0, \ldots, 0)&lt;/script&gt; resulting in a score of correct entries &lt;script type=&quot;math/tex&quot;&gt;s_0&lt;/script&gt; and then, for each &lt;script type=&quot;math/tex&quot;&gt;i = 1, \ldots, n&lt;/script&gt; guess &lt;script type=&quot;math/tex&quot;&gt;\vec{g}_i&lt;/script&gt; – the binary vector with only one &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; in position &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; be the resulting score. If &lt;script type=&quot;math/tex&quot;&gt;s_i &gt; s_0&lt;/script&gt;, then the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th entry of &lt;script type=&quot;math/tex&quot;&gt;\vec{k}&lt;/script&gt; must be a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;. Otherwise it is a &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;While this is certainly possible to pull off in practice &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, it &lt;em&gt;does not&lt;/em&gt; satisfy my urge for efficiency.&lt;/p&gt;

&lt;p&gt;Can we do better?&lt;/p&gt;

&lt;p&gt;Indeed! But making significant progress will require some work.&lt;/p&gt;

&lt;h2 id=&quot;insert-graph-theory&quot;&gt;Insert Graph Theory&lt;/h2&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;G = (V;E)&lt;/script&gt; be an undirected &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;&gt;graph&lt;/a&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;u,v \in V&lt;/script&gt; be vertices. The &lt;em&gt;distance &lt;script type=&quot;math/tex&quot;&gt;d_G(v,u)&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;&lt;/em&gt; in &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; (in symbols &lt;script type=&quot;math/tex&quot;&gt;d_G(u,v)&lt;/script&gt;) is the length of the minimal path of edges in &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; that connects &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;. If no such path exists, we let &lt;script type=&quot;math/tex&quot;&gt;d_G(v,u) := \infty&lt;/script&gt;. &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/diagrams/distance.png&quot; alt=&quot;distance of two nodes in a graph&quot; /&gt;
  &lt;figcaption&gt;A shortest path of length 2 between u and v&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;definition-1&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;G = (V;E)&lt;/script&gt; be an undirected graph and let &lt;script type=&quot;math/tex&quot;&gt;R \subseteq V&lt;/script&gt; be a set of vertices. &lt;em&gt;&lt;script type=&quot;math/tex&quot;&gt;R = \{r_1, \ldots, r_k \}&lt;/script&gt; resolves &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;&lt;/em&gt; if every vertex &lt;script type=&quot;math/tex&quot;&gt;v \in V&lt;/script&gt; is uniquely determined by its vector of distances to members of &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;. To put it in mathematical terms: &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; resolves &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; if the function&lt;/p&gt;

&lt;p&gt;\[
d_R \colon V \to [0, \infty]^k, v \mapsto (d_G(v,r_1), d_G(v,r_2), \ldots, d_G(v,r_k))
\]&lt;/p&gt;

&lt;p&gt;is &lt;a href=&quot;https://en.wikipedia.org/wiki/Injective_function&quot;&gt;injective&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;R = V = \{ v_1, \ldots, v_n \}&lt;/script&gt; trivially resolves &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; as every &lt;script type=&quot;math/tex&quot;&gt;v \in V&lt;/script&gt; is uniquely determined by &lt;script type=&quot;math/tex&quot;&gt;i \in \{1, \ldots, n \}&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;d_G(v,v_i) = 0&lt;/script&gt; since in this case we have &lt;script type=&quot;math/tex&quot;&gt;v = v_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Consider the following example:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/diagrams/resolving_set.png&quot; alt=&quot;a resolving set of size 4 for H(3)&quot; /&gt;
  &lt;figcaption&gt;A resolving set of size 3 for H(3)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Here, the vertex &lt;script type=&quot;math/tex&quot;&gt;(0,1,1)&lt;/script&gt; is uniquely identified by having distance &lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt; to  &lt;script type=&quot;math/tex&quot;&gt;r_1 = (0,0,0)&lt;/script&gt;, distance &lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;r_2 = (1,0,0)&lt;/script&gt; and distance &lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;r_3 = (1,1,0)&lt;/script&gt;. In other words, &lt;script type=&quot;math/tex&quot;&gt;(0,1,1)&lt;/script&gt; is the unique vertex with distance vector &lt;script type=&quot;math/tex&quot;&gt;d_R(0,1,1) = (2,3,2)&lt;/script&gt; to the highlighted resolving set.&lt;/p&gt;

&lt;p&gt;I encourage the reader to check that &lt;script type=&quot;math/tex&quot;&gt;R = \{ (0,0,0), (1,0,0), (1,1,0) \}&lt;/script&gt; is indeed a resolving set for &lt;script type=&quot;math/tex&quot;&gt;H(3)&lt;/script&gt; to get a better feel for this rather abstract concept before moving on.&lt;/p&gt;

&lt;h3 id=&quot;definition-2&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;G = (V;E)&lt;/script&gt; be an undirected graph. The &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_dimension_(graph_theory)&quot;&gt;metric dimension&lt;/a&gt; of &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;&lt;/em&gt;  is the minimal size of some &lt;script type=&quot;math/tex&quot;&gt;R \subseteq V&lt;/script&gt; that resolves &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Returning to our example &lt;script type=&quot;math/tex&quot;&gt;H(3)&lt;/script&gt;: We’ve already found a resolving set of size &lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt; and a bit of computation confirms that there is no resolving set of size &lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt;. Therefore the metric dimension of &lt;script type=&quot;math/tex&quot;&gt;H(3)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;It is now time to meet the hero of our advanced guessing strategy.&lt;/p&gt;

&lt;h3 id=&quot;definition-3&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;The &lt;em&gt;&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-dimensional &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_graph&quot;&gt;Hamming graph&lt;/a&gt;&lt;/em&gt; is the undirected graph &lt;script type=&quot;math/tex&quot;&gt;H(n) = (V;E)&lt;/script&gt; whose vertices are all &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-dimensional binary vectors &lt;script type=&quot;math/tex&quot;&gt;\vec{v} = (v_1, \ldots, v_n)&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;\{\vec{v},\vec{u}\} \in E&lt;/script&gt; iff they differ in exactly one position, i.e.&lt;/p&gt;

&lt;p&gt;\[
E = \{ \{ \vec{v}, \vec{u} \} \mid \{i \mid v_i \neq u_i \} \text{ has size 1 } \}.
\]&lt;/p&gt;

&lt;p&gt;If you look back to the diagram of &lt;script type=&quot;math/tex&quot;&gt;H(3)&lt;/script&gt;, you will see that this is indeed the &lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;-dimensional Hamming graph. Its vertices are binary vectors of length &lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt; and they are connected via a single edge if and only if they differ in exactly one coordinate.&lt;/p&gt;

&lt;h2 id=&quot;a-graph-theoretic-guessing-strategy&quot;&gt;A Graph Theoretic Guessing Strategy&lt;/h2&gt;

&lt;p&gt;Here is our graph theoretic guessing strategy: Let &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; be a small resolving set for &lt;script type=&quot;math/tex&quot;&gt;H(n)&lt;/script&gt;. Submit each &lt;script type=&quot;math/tex&quot;&gt;\vec{r} \in R&lt;/script&gt; as a guess to Kaggle which sends back its score &lt;script type=&quot;math/tex&quot;&gt;s(\vec{r})&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;s(\vec{r}) = n&lt;/script&gt; for some &lt;script type=&quot;math/tex&quot;&gt;\vec{r}&lt;/script&gt;, we’ve achieved a perfect score and are done with this competition. Otherwise, since &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; is a resolving set, there is a unique vertex &lt;script type=&quot;math/tex&quot;&gt;\vec{g}&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;H(n)&lt;/script&gt; such that&lt;/p&gt;

&lt;p&gt;\[
d_{H(n)}(\vec{g},\vec{r}) = n - s(\vec{r})
\]&lt;/p&gt;

&lt;p&gt;for all &lt;script type=&quot;math/tex&quot;&gt;\vec{r} \in R&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;However, by the construction of &lt;script type=&quot;math/tex&quot;&gt;H(n)&lt;/script&gt;, we have &lt;script type=&quot;math/tex&quot;&gt;d_{H(n)} (\vec{k},\vec{r}) = n - s(\vec{r})&lt;/script&gt; (where &lt;script type=&quot;math/tex&quot;&gt;\vec{k}&lt;/script&gt; is Kaggle’s secret solution vector) for all &lt;script type=&quot;math/tex&quot;&gt;\vec{r} \in R&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;\vec{g}&lt;/script&gt; is the &lt;em&gt;unique&lt;/em&gt; vector with this property, we must have that &lt;script type=&quot;math/tex&quot;&gt;\vec{g} = \vec{k}&lt;/script&gt; is the desired solution. This strategy therefore guarantees a perfect score in at most &lt;script type=&quot;math/tex&quot;&gt;\mathrm{size(R)} + 1&lt;/script&gt; many submissions!&lt;/p&gt;

&lt;p&gt;Now, because &lt;script type=&quot;math/tex&quot;&gt;H(n)&lt;/script&gt; has a metric dimension of &lt;script type=&quot;math/tex&quot;&gt;\frac{(2+ o(1))n}{\log_2(n)}&lt;/script&gt;, we will be able to crack the Titanic dataset in &lt;script type=&quot;math/tex&quot;&gt;\sim 97&lt;/script&gt; &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; submissions and furthermore, if we commit to submitting all but the final guess at once, this is known to be an optimal guessing strategy.&lt;/p&gt;

&lt;h2 id=&quot;confessions&quot;&gt;Confessions&lt;/h2&gt;

&lt;p&gt;While this graph theoretic approach to beat Kaggle, from a purely mathematical point of view, is very pleasing, it doesn’t seem that practical after all. For instance, while the metric dimension of &lt;script type=&quot;math/tex&quot;&gt;H(n)&lt;/script&gt; is known asymptotically, &lt;a href=&quot;https://mathoverflow.net/questions/332434/explicit-small-resolving-sets-for-hamming-graphs&quot;&gt;it’s not clear to me how to find small resolving sets for &lt;script type=&quot;math/tex&quot;&gt;H(n)&lt;/script&gt; in practice&lt;/a&gt;. Furthermore, even if you were given a small resolving set &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;, calculating &lt;script type=&quot;math/tex&quot;&gt;\vec{k}&lt;/script&gt; from &lt;script type=&quot;math/tex&quot;&gt;\{ d_{H(n)}(\vec{k}, \vec{r}) \mid \vec{r} \in R \}&lt;/script&gt; requires some serious computational power. Granted, it doesn’t increase the number of guesses required, the metric I’ve chosen to optimize for, but it still results in so much computational overhead that the naive approach will win out.&lt;/p&gt;

&lt;p&gt;What’s worse: We are not taking full advantage of Kaggle’s feedback to our guesses. When guessing the entries of our resolving set one by one, we don’t use the information gained to guide our future guesses. Instead we could just as well have submitted them all in one go, collect the scores and then compute the correct solution. If we add this further restriction, the solution presented here is optimal. However, without this artificial restriction, I suspect that a better performing, general solution should be possible and I’d very much like to find one.&lt;/p&gt;

&lt;p&gt;In practice, if you want to do better than this graph theoretic guessing strategy, I’d suggest cooking up a reasonably well-performing model. You then adapt the naive approach by prioritizing those bits that your model is least certain about. This, if I had to guess, is what people actually do to obtain perfect scores. Finally, to get a perfect score with a single entry, just calculate the solution on a different account and, once you’ve obtained it, submit it on your main account.&lt;/p&gt;

&lt;p&gt;From Kaggle’s point of view, at least in the case of the Titanic dataset, this attack vector is pretty much impossible to defend against. For larger datasets, on the other hand, there certainly is a lot they can do to prevent successful guessing strategies. That, however, is a story for another day…&lt;/p&gt;

&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mathoverflow.net/questions/58600&quot;&gt;Math Overflow. Guessing a subset of &lt;script type=&quot;math/tex&quot;&gt;\{1, \ldots, N \}&lt;/script&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://epubs.siam.org/doi/pdf/10.1137/1.9781611975482.74&quot;&gt;Jian, Polyanskii. How to guess an &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-digit number&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1712.02723.pdf&quot;&gt;Jian, Polyanksii. On the metric dimension of Cartesian powers of a graph&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/math/0507527.pdf&quot;&gt;Caceres, Puertas. ON THE METRIC DIMENSION OFCARTESIAN PRODUCTS OF GRAPHS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/math/0507527.pdf&quot;&gt;Caceres, Hernando, Mora, Pelayo, Puertas, Seara and Wood. ON THE METRIC DIMENSION OF CARTESIAN PRODUCTS OF GRAPHS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bernt Lindström.  On a combinatory detection problem. I (Magyar Tud. Akad.Mat. Kutato Int. Közl., 9:195–207, 1964)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mathoverflow.net/questions/332434/explicit-small-resolving-sets-for-hamming-graphs&quot;&gt;Math Overflow. Explicit, small resolving sets for Hamming graphs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Kaggle’s daily submission limit poses only the tiniest of hurdles to any programmer… &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;d_G&lt;/script&gt; is the usual &lt;a href=&quot;https://en.wikipedia.org/wiki/Distance_(graph_theory)&quot;&gt;graph metric&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;The exact number depends on the precise metric dimension of &lt;script type=&quot;math/tex&quot;&gt;H(418)&lt;/script&gt; which I do not yet know at the time of writing this post. All I know for certain is that it is &lt;script type=&quot;math/tex&quot;&gt;\le 418&lt;/script&gt;. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stefan Mesken</name></author><category term="Machine Learning" /><category term="machine learning" /><category term="mathematics" /><category term="graph theory" /><summary type="html">A few nights ago, I found myself tinkering with the Titanic data set on Kaggle and couldn’t help but notice the number of people with a perfect score – many of whom have a single entry.</summary></entry></feed>